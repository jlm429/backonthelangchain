{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72834f6-f209-4a48-ae0d-8b1ee186dc59",
   "metadata": {},
   "source": [
    "## Reflexion-Inspired LLM Agent with LangGraph and Tavily\n",
    "\n",
    "This notebook builds a multi-step reasoning agent using [LangGraph](https://github.com/langchain-ai/langgraph), inspired by the **Reflexion** framework:\n",
    "\n",
    "**Reflexion: Language Agents with Verbal Reinforcement Learning**  \n",
    "Yao et al. (2023) – [arXiv link](https://arxiv.org/abs/2303.11366)\n",
    "\n",
    "The agent follows a 3-step reasoning loop:\n",
    "1. **Answer** – Generate an initial response.\n",
    "2. **Reflect** – Critique what’s missing or unnecessary.\n",
    "3. **Revise** – Improve the answer using critique and web search.\n",
    "\n",
    "Search is powered by [Tavily](https://www.tavily.com/), enabling the agent to query the web based on its own reflection.\n",
    "\n",
    "> Also influenced by ideas from the LangGraph Udemy course: https://www.udemy.com/course/langgraph\n",
    "\n",
    "At the end of each run, the final answer and total token usage are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a852bf3-e20c-4baf-8aa3-43d1ea7c2cdf",
   "metadata": {},
   "source": [
    "### Answer and Reflection Schema\n",
    "\n",
    "This cell defines structured Pydantic models for a multi-step Q&A workflow:\n",
    "\n",
    "- **`Reflection`** captures what’s missing or unnecessary in an answer.\n",
    "- **`AnswerQuestion`** includes a ~250-word response, a reflection, and 1–3 search queries.\n",
    "- **`ReviseAnswer`** extends this with a list of supporting references.\n",
    "\n",
    "These schemas enable structured parsing in LangChain or agent-based pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a507e64d-0b04-43cb-b0ad-95fc54a7dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary types and modules\n",
    "from typing import List\n",
    "from dotenv import load_dotenv  # Loads environment variables from a .env file\n",
    "from pydantic import BaseModel, Field  # For data validation and schema enforcement\n",
    "\n",
    "# Load environment variables (e.g., API keys)\n",
    "load_dotenv()\n",
    "\n",
    "# Define a schema for critique/reflection on a generated answer\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")  # Feedback on missing content\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")  # Feedback on unnecessary/excess detail\n",
    "\n",
    "# Schema for answering a question with structured components\n",
    "class AnswerQuestion(BaseModel):\n",
    "    \"\"\"Answer the question.\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"~250 word detailed answer to the question.\")  # Main response text\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")  # Self-critique of the answer\n",
    "    search_queries: List[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )  # Optional tool input to help guide future revisions\n",
    "\n",
    "# Schema for a revised answer that includes supporting citations\n",
    "class ReviseAnswer(AnswerQuestion):\n",
    "    \"\"\"Revise your original answer to your question.\"\"\"\n",
    "\n",
    "    references: List[str] = Field(\n",
    "        description=\"Citations motivating your updated answer.\"\n",
    "    )  # List of reference URLs or sources used to support the revised answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca0de9-1f85-4472-84c0-9cece172b054",
   "metadata": {},
   "source": [
    "### Tool Configuration for Answer and Revision Agents\n",
    "\n",
    "This cell sets up the language model, prompt templates, and tool bindings for the answer–reflect–revise workflow.\n",
    "\n",
    "- **Model and Parsers**:\n",
    "  - Initializes an OpenAI-compatible LLM.\n",
    "  - Uses `JsonOutputToolsParser` for general tool output and `PydanticToolsParser` for structured `AnswerQuestion` responses.\n",
    "\n",
    "- **Prompt Template**:\n",
    "  - Defines a shared `actor_prompt_template` with instructions to answer, critique, and suggest search queries.\n",
    "  - Includes a `MessagesPlaceholder` for chat history and injects the current time dynamically.\n",
    "\n",
    "- **Tool Bindings**:\n",
    "  - **`first_responder`**: Generates an initial answer and critique using the base prompt and `AnswerQuestion` tool.\n",
    "  - **`revisor`**: Revises the answer using custom instructions and the `ReviseAnswer` tool.\n",
    "\n",
    "These components enable structured, multi-step reasoning in a LangGraph pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af53d81f-c082-4c5b-962d-93a8b24a59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary LangChain classes and parsers\n",
    "from langchain_core.messages import HumanMessage  # Used for user input in message history\n",
    "from langchain_core.output_parsers.openai_tools import (\n",
    "    JsonOutputToolsParser,     # Generic JSON output parser for OpenAI tool outputs\n",
    "    PydanticToolsParser,       # Parser that converts tool output into Pydantic models\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  # Used to define LLM prompt templates\n",
    "from langchain_openai import ChatOpenAI  # OpenAI chat model wrapper for LangChain\n",
    "\n",
    "from schemas import AnswerQuestion, ReviseAnswer  # Custom Pydantic schemas for tool outputs\n",
    "\n",
    "# Initialize the LLM with a specific model (OpenAI-compatible)\n",
    "llm = ChatOpenAI(model=\"o4-mini\")\n",
    "\n",
    "# Parser that returns tool outputs as JSON along with the tool name/id\n",
    "parser = JsonOutputToolsParser(return_id=True)\n",
    "\n",
    "# Parser that converts the tool output into structured Pydantic objects (like AnswerQuestion)\n",
    "parser_pydantic = PydanticToolsParser(tools=[AnswerQuestion])\n",
    "\n",
    "# Prompt template used by both first-responder and reviser agents\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert researcher.\n",
    "Current time: {time}\n",
    "\n",
    "1. {first_instruction}\n",
    "2. Reflect critically on your answer. What key points are missing? What content could be removed?\n",
    "3. Recommend search queries to research information and improve your answer.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # Used to include previous chat history dynamically\n",
    "        (\"system\", \"Answer the user's question above using the required format.\"),  # Explicit instruction to format output properly\n",
    "    ]\n",
    ").partial(\n",
    "    time=lambda: datetime.datetime.now().isoformat(),  # Injects current timestamp dynamically into the prompt\n",
    ")\n",
    "\n",
    "# First responder configuration: focuses on generating an initial answer\n",
    "first_responder_prompt_template = actor_prompt_template.partial(\n",
    "    first_instruction=\"Provide a detailed ~250 word answer.\"\n",
    ")\n",
    "\n",
    "# Chain that binds the first_responder prompt to the model and specifies tool output\n",
    "first_responder = first_responder_prompt_template | llm.bind_tools(\n",
    "    tools=[AnswerQuestion], tool_choice=\"AnswerQuestion\"\n",
    ")\n",
    "\n",
    "# Instruction text for revision agent, guiding how to update the original answer\n",
    "revise_instructions = \"\"\"Revise your previous answer using the new information.\n",
    "    - You should use the previous critique to add important information to your answer.\n",
    "        - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
    "        - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit). In form of:\n",
    "            - [1] https://example.com\n",
    "            - [2] https://example.com\n",
    "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
    "\"\"\"\n",
    "\n",
    "# Revisor configuration: modifies the original answer using critique and new info\n",
    "revisor = actor_prompt_template.partial(\n",
    "    first_instruction=revise_instructions\n",
    ") | llm.bind_tools(tools=[ReviseAnswer], tool_choice=\"ReviseAnswer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b47aa61-0933-45fb-a3b5-1ce39c8ff673",
   "metadata": {},
   "source": [
    "### LangGraph Construction and Execution\n",
    "\n",
    "This cell builds and runs a LangGraph agent for multi-step reasoning and answer refinement.\n",
    "\n",
    "- **Graph Definition**:\n",
    "  - Constructs a `MessageGraph` with three nodes:\n",
    "    - **`draft`**: generates the initial answer using `first_responder`.\n",
    "    - **`execute_tools`**: runs search queries via the tool executor.\n",
    "    - **`revise`**: revises the answer using `revisor`.\n",
    "  - Edges define the flow: `draft → execute_tools → revise`.\n",
    "  - A conditional loop enforces a max of `MAX_ITERATIONS` through the revision cycle based on `ToolMessage` count.\n",
    "\n",
    "- **Graph Execution**:\n",
    "  - Accepts user input as a prompt.\n",
    "  - Executes the graph and extracts the final answer (from tool output if present).\n",
    "  - Prints both the final answer and token usage for monitoring model cost.\n",
    "\n",
    "This ties together the model, prompts, and tools into a complete answer–reflect–revise pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b29352ee-e4fd-472c-9f8a-fdbf1088f1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:   Write about using agentic approaches to solving tier 1 IT and HR issues and list startups that realm that have raised capital.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reflion agent:   Write about using agentic approaches to solving tier 1 IT and HR issues and list startups that realm that have raised capital.\n",
      "\n",
      "=== Final Answer ===\n",
      "\n",
      "Agentic AI—autonomous, goal-directed software agents powered by large language models—automates up to 70% of Tier 1 IT and HR tasks by orchestrating microservices, event-driven pipelines and continuous feedback loops[1]. In enterprise pilots, these agents boosted first-contact resolution from 30% to 65% and cut mean time to resolution by 45% versus legacy rule-based bots[2].\n",
      "\n",
      "IT Use Cases: Agents triage tickets, reset credentials, provision accounts and diagnose routine network or software issues via integrations with ServiceNow, Jira and identity platforms (e.g., Okta). Confidence thresholds guide escalations to human engineers.\n",
      "\n",
      "HR Use Cases: Agents answer benefits FAQs, schedule interviews, onboard new hires and process time-off or payroll inquiries by interfacing with Workday, Greenhouse and calendar APIs—delivering 24/7 support and reducing HR ticket volume by 40%[2].\n",
      "\n",
      "Notable Startups (Institutional Funding):\n",
      "• Moveworks ($225 M) – AI-driven IT service desk automation[3]\n",
      "• Paradox.ai ($80 M) – Conversational recruiting and employee engagement[4]\n",
      "• Mya Systems ($40 M) – LLM-based talent screening assistant[5]\n",
      "• Talla ($10 M) – AI knowledge-management bots for IT/HR[3]\n",
      "• Forethought (Agatha) ($60 M) – Automated helpdesk ticket triage[2]\n",
      "\n",
      "Embedding agentic AI into existing enterprise ecosystems yields scalable, data-driven support operations, measurable ROI and improved employee satisfaction.\n",
      "\n",
      "--- Token Usage ---\n",
      "Total tokens: 11690\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# LangGraph and LangChain imports for message handling and agent graph definition\n",
    "from langchain_core.messages import BaseMessage, ToolMessage, AIMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "# Import the nodes (first_responder, revisor) and tool executor function from other modules\n",
    "from chains import revisor, first_responder\n",
    "from tool_executor import execute_tools\n",
    "\n",
    "# Maximum number of tool-use iterations to prevent infinite loops\n",
    "MAX_ITERATIONS = 2\n",
    "\n",
    "# === Build the LangGraph ===\n",
    "builder = MessageGraph()\n",
    "\n",
    "# Add nodes (graph steps)\n",
    "builder.add_node(\"draft\", first_responder)          # Step 1: Generate initial answer\n",
    "builder.add_node(\"execute_tools\", execute_tools)    # Step 2: Run any tool calls\n",
    "builder.add_node(\"revise\", revisor)                 # Step 3: Revise the answer based on critique\n",
    "\n",
    "# Define edges (step transitions)\n",
    "builder.add_edge(\"draft\", \"execute_tools\")          # After initial draft, go to tool execution\n",
    "builder.add_edge(\"execute_tools\", \"revise\")         # After tools run, go to revision\n",
    "\n",
    "# Conditional loop: determines whether to stop or return to tool execution\n",
    "def event_loop(state: List[BaseMessage]) -> str:\n",
    "    # Count how many times tools have been used\n",
    "    count_tool_visits = sum(isinstance(item, ToolMessage) for item in state)\n",
    "    if count_tool_visits > MAX_ITERATIONS:\n",
    "        return END  # Stop the loop if limit exceeded\n",
    "    return \"execute_tools\"  # Otherwise, keep revising via tool execution\n",
    "\n",
    "# Attach conditional edge from \"revise\" step using event_loop logic\n",
    "builder.add_conditional_edges(\"revise\", event_loop)\n",
    "\n",
    "# Set the entry point for the graph\n",
    "builder.set_entry_point(\"draft\")\n",
    "\n",
    "# Compile the graph into an executable object\n",
    "graph = builder.compile()\n",
    "\n",
    "# === User Input & Execution ===\n",
    "\n",
    "# Get prompt from user\n",
    "user_prompt = input(\"Enter your question: \")\n",
    "print(\"Running reflion agent: \", user_prompt)\n",
    "\n",
    "# Run the graph agent with user input\n",
    "res = graph.invoke(user_prompt)\n",
    "\n",
    "# === Parse final output ===\n",
    "\n",
    "# Extract the final answer:\n",
    "# If it's an AIMessage with a tool call, grab the tool's output argument\n",
    "if isinstance(res[-1], AIMessage) and res[-1].tool_calls:\n",
    "    final_answer = res[-1].tool_calls[0][\"args\"].get(\"answer\", \"\")\n",
    "else:\n",
    "    # Otherwise, fall back to the message content directly\n",
    "    final_answer = res[-1].content if hasattr(res[-1], \"content\") else str(res[-1])\n",
    "\n",
    "# === Token usage reporting (optional) ===\n",
    "\n",
    "# Try to extract token usage info from response metadata\n",
    "token_usage = res[-1].response_metadata.get(\"token_usage\", {}) if hasattr(res[-1], 'response_metadata') else {}\n",
    "total_tokens = token_usage.get(\"total_tokens\", \"N/A\")\n",
    "\n",
    "# === Output results ===\n",
    "print(\"\\n=== Final Answer ===\\n\")\n",
    "print(final_answer)\n",
    "\n",
    "print(f\"\\n--- Token Usage ---\\nTotal tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4bb5db-38a3-4489-b40e-9a3cf363462a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain (Pipenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
